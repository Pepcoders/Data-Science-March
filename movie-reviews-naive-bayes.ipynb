{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-08T15:40:41.276486Z","iopub.execute_input":"2022-06-08T15:40:41.276815Z","iopub.status.idle":"2022-06-08T15:40:41.280531Z","shell.execute_reply.started":"2022-06-08T15:40:41.276782Z","shell.execute_reply":"2022-06-08T15:40:41.279982Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\ndf.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-06-08T15:40:45.691014Z","iopub.execute_input":"2022-06-08T15:40:45.691371Z","iopub.status.idle":"2022-06-08T15:40:46.485080Z","shell.execute_reply.started":"2022-06-08T15:40:45.691337Z","shell.execute_reply":"2022-06-08T15:40:46.484300Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T15:41:09.191382Z","iopub.execute_input":"2022-06-08T15:41:09.191680Z","iopub.status.idle":"2022-06-08T15:41:09.211716Z","shell.execute_reply.started":"2022-06-08T15:41:09.191651Z","shell.execute_reply":"2022-06-08T15:41:09.210784Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df['sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T15:41:29.380859Z","iopub.execute_input":"2022-06-08T15:41:29.381412Z","iopub.status.idle":"2022-06-08T15:41:29.393471Z","shell.execute_reply.started":"2022-06-08T15:41:29.381369Z","shell.execute_reply":"2022-06-08T15:41:29.392772Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"#### So we can see there are 2 columns - review and sentiment. sentiment is the target column that we need to predict. The dataset is completely balanced and it has equal number of positive and negative sentiments.","metadata":{}},{"cell_type":"markdown","source":"#### Let's take one review as sample and understand why we need to clean the text.","metadata":{}},{"cell_type":"code","source":"review = df['review'].loc[1]\nreview","metadata":{"execution":{"iopub.status.busy":"2022-06-08T16:10:46.597058Z","iopub.execute_input":"2022-06-08T16:10:46.597410Z","iopub.status.idle":"2022-06-08T16:10:46.603282Z","shell.execute_reply.started":"2022-06-08T16:10:46.597372Z","shell.execute_reply":"2022-06-08T16:10:46.602517Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"#### Normally any NLP task involves following text cleaning techniques -\n\n1. Removal of HTML contents like \"< br>\".\n2. Removal of punctutions, special characters like '\\'.\n3. Removal of stopwords like is, the which do not offer much insight.\n4. Stemming/Lemmatization to bring back multiple forms of same word to their common root like 'coming', 'comes' into 'come'.\n5. Vectorization - Encode the numeric values once you have cleaned it.\n6. Fit the data to the ML model.\n\n#### We will apply all these techniques on this sample review and understand how it works.\n\n#### First of all we will remove HTML contents.","metadata":{}},{"cell_type":"markdown","source":"#### Removal of HTML contents like \"< br>\".","metadata":{}},{"cell_type":"code","source":"from bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(review, \"html.parser\")\nreview = soup.get_text()\nreview","metadata":{"execution":{"iopub.status.busy":"2022-06-08T16:10:48.795880Z","iopub.execute_input":"2022-06-08T16:10:48.796416Z","iopub.status.idle":"2022-06-08T16:10:48.802663Z","shell.execute_reply.started":"2022-06-08T16:10:48.796361Z","shell.execute_reply":"2022-06-08T16:10:48.802144Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"#### We can see HTML tags are removed; so in the next step we will remove everything except lower/upper case letters using Regular Expressions.","metadata":{}},{"cell_type":"code","source":"import re\n\nreview = re.sub('\\[[^]]*\\]', ' ', review)\nreview = re.sub('[^a-zA-Z]', ' ', review)\nreview","metadata":{"execution":{"iopub.status.busy":"2022-06-08T16:10:51.985889Z","iopub.execute_input":"2022-06-08T16:10:51.986348Z","iopub.status.idle":"2022-06-08T16:10:51.992602Z","shell.execute_reply.started":"2022-06-08T16:10:51.986309Z","shell.execute_reply":"2022-06-08T16:10:51.991982Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"#### Next we will bring everything into lowercase.","metadata":{}},{"cell_type":"code","source":"review = review.lower()\nprint(review)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T16:10:56.596413Z","iopub.execute_input":"2022-06-08T16:10:56.596945Z","iopub.status.idle":"2022-06-08T16:10:56.602330Z","shell.execute_reply.started":"2022-06-08T16:10:56.596883Z","shell.execute_reply":"2022-06-08T16:10:56.601137Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"#### Stopwords removal - since stopwords removal works on every word in your text we need to split the text.","metadata":{}},{"cell_type":"code","source":"review = review.split()\nprint(len(review))\nprint(review)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T16:11:00.455989Z","iopub.execute_input":"2022-06-08T16:11:00.456314Z","iopub.status.idle":"2022-06-08T16:11:00.461255Z","shell.execute_reply.started":"2022-06-08T16:11:00.456284Z","shell.execute_reply":"2022-06-08T16:11:00.460339Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\n\nfor word in set(stopwords.words('english')):\n    print(word, end=\" | \")","metadata":{"execution":{"iopub.status.busy":"2022-06-08T16:09:30.941491Z","iopub.execute_input":"2022-06-08T16:09:30.941858Z","iopub.status.idle":"2022-06-08T16:09:30.954262Z","shell.execute_reply.started":"2022-06-08T16:09:30.941824Z","shell.execute_reply":"2022-06-08T16:09:30.953257Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"review = [word for word in review if not word in set(stopwords.words('english'))]\nprint(len(review))\nprint(review)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T16:11:12.330799Z","iopub.execute_input":"2022-06-08T16:11:12.331289Z","iopub.status.idle":"2022-06-08T16:11:12.366532Z","shell.execute_reply.started":"2022-06-08T16:11:12.331254Z","shell.execute_reply":"2022-06-08T16:11:12.365898Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"#### Stemming/Lemmatization - we will apply both and see the difference.","metadata":{}},{"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer\n\nps = PorterStemmer()\nreview_s = [ps.stem(word) for word in review]\nprint(review_s)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T16:11:37.641968Z","iopub.execute_input":"2022-06-08T16:11:37.642345Z","iopub.status.idle":"2022-06-08T16:11:37.652877Z","shell.execute_reply.started":"2022-06-08T16:11:37.642309Z","shell.execute_reply":"2022-06-08T16:11:37.652029Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\n\nlem = WordNetLemmatizer()\nreview_lemmat = [lem.lemmatize(word) for word in review]\nprint(review_lemmat)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T16:11:49.751195Z","iopub.execute_input":"2022-06-08T16:11:49.751485Z","iopub.status.idle":"2022-06-08T16:11:51.322080Z","shell.execute_reply.started":"2022-06-08T16:11:49.751457Z","shell.execute_reply":"2022-06-08T16:11:51.320370Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"#### We can see that 'little' has become 'littl' after Stemming but remained 'little' after Lemmatization. We will use Lemmatization.","metadata":{}},{"cell_type":"markdown","source":"#### Merge the words to form cleaned up version of the text.","metadata":{}},{"cell_type":"code","source":"review = ' '.join(review_lemmat)\nreview","metadata":{"execution":{"iopub.status.busy":"2022-06-08T16:13:54.666740Z","iopub.execute_input":"2022-06-08T16:13:54.667070Z","iopub.status.idle":"2022-06-08T16:13:54.672742Z","shell.execute_reply.started":"2022-06-08T16:13:54.667040Z","shell.execute_reply":"2022-06-08T16:13:54.671983Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"#### Our next step will be to bring this text in mathematical forms and to do so we will create a Corpus first.","metadata":{}},{"cell_type":"code","source":"corpus = []\ncorpus.append(review)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T16:15:22.096547Z","iopub.execute_input":"2022-06-08T16:15:22.096997Z","iopub.status.idle":"2022-06-08T16:15:22.101305Z","shell.execute_reply.started":"2022-06-08T16:15:22.096958Z","shell.execute_reply":"2022-06-08T16:15:22.100336Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"#### To vectorize the text we will apply -\n\n1. CountVectorizer (Bag of Words Model)\n2. TfidfVectorizer (Bag of Words Model)\n3. Keras Tokenizer (Embedding)","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vec = CountVectorizer()\nreview_count_vec = count_vec.fit_transform(corpus)\n\nreview_count_vec.toarray()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T16:15:26.131265Z","iopub.execute_input":"2022-06-08T16:15:26.131856Z","iopub.status.idle":"2022-06-08T16:15:26.138884Z","shell.execute_reply.started":"2022-06-08T16:15:26.131817Z","shell.execute_reply":"2022-06-08T16:15:26.138375Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"#### So we can see the data has become numeric with 1,2 and 3s based on the number of times they appear in the text.\n\n#### There is another variation of CountVectorizer with binary=True and in that case all zero entries will have 1.","metadata":{}},{"cell_type":"code","source":"count_vec_bin = CountVectorizer(binary=True)\nreview_count_vec_bin = count_vec_bin.fit_transform(corpus)\n\nreview_count_vec_bin.toarray()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T16:16:14.862553Z","iopub.execute_input":"2022-06-08T16:16:14.863129Z","iopub.status.idle":"2022-06-08T16:16:14.869996Z","shell.execute_reply.started":"2022-06-08T16:16:14.863093Z","shell.execute_reply":"2022-06-08T16:16:14.869009Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"#### So there is no 2s and 3s in the vector.\n\n#### We will now explore TF-IDF - TF stands for Text Frequency which means how many times a word (term) appears in a text (document). IDF means Inverse Document Frequency and is calculated as log(# of documents in corpus/# of documents containing the term).\n\n#### Finally TF-IDF score is calculated as TF * IDF.\n\n#### IDF acts as a balancing factor and diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vec = TfidfVectorizer()\nreview_tfidf_vec = tfidf_vec.fit_transform(corpus)\n\nreview_tfidf_vec.toarray()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T16:19:14.952195Z","iopub.execute_input":"2022-06-08T16:19:14.952585Z","iopub.status.idle":"2022-06-08T16:19:14.969411Z","shell.execute_reply.started":"2022-06-08T16:19:14.952546Z","shell.execute_reply":"2022-06-08T16:19:14.968584Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T16:19:34.166391Z","iopub.execute_input":"2022-06-08T16:19:34.166703Z","iopub.status.idle":"2022-06-08T16:19:34.175171Z","shell.execute_reply.started":"2022-06-08T16:19:34.166673Z","shell.execute_reply":"2022-06-08T16:19:34.174505Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"df[\"sentiment\"] = df[\"sentiment\"].replace({\"positive\": 1, \"negative\":0})\ndf[\"sentiment\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T16:19:55.266956Z","iopub.execute_input":"2022-06-08T16:19:55.267457Z","iopub.status.idle":"2022-06-08T16:19:55.307283Z","shell.execute_reply.started":"2022-06-08T16:19:55.267419Z","shell.execute_reply":"2022-06-08T16:19:55.306375Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"X = df[\"review\"]\ny = df[\"sentiment\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-08T16:20:12.046664Z","iopub.execute_input":"2022-06-08T16:20:12.047027Z","iopub.status.idle":"2022-06-08T16:20:12.050785Z","shell.execute_reply.started":"2022-06-08T16:20:12.046995Z","shell.execute_reply":"2022-06-08T16:20:12.050167Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"data_corpus = []\n\nfor i in range(X.shape[0]):\n    soup = BeautifulSoup(X.iloc[i], \"html.parser\")\n    review = soup.get_text()\n    review = re.sub('\\[[^]]*\\]', ' ', review)\n    review = re.sub('[^a-zA-Z]', ' ', review)\n    review = review.lower()\n    review = review.split()\n    review = [word for word in review if not word in set(stopwords.words('english'))]\n    lem = WordNetLemmatizer()\n    review = [lem.lemmatize(word) for word in review]\n    review = ' '.join(review)\n    data_corpus.append(review)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T16:24:58.656136Z","iopub.execute_input":"2022-06-08T16:24:58.656453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's validate one sample entry.","metadata":{}},{"cell_type":"code","source":"data_corpus[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We will now fit the data to Naive Bayes classifier. \n\n#### Bayesian model uses prior probabilities to predict posterior probabilites which is helpful for classification with discrete features like text classification.\n\n#### Here we are using Multinomial Naive Bayes try other Naive Bayes","metadata":{}},{"cell_type":"markdown","source":"#### The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work. We will now use CountVectorizer as our Bag-of-Words model before applying MultinomialNB model.","metadata":{}},{"cell_type":"code","source":"count_vec_NB = CountVectorizer(ngram_range=(1, 3), binary=False)\ncount_vec_data_NB = count_vec_NB.fit_transform(data_corpus)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### So there are 900000 terms in the corpus and we will use a *Chi-Square* test to select top 5000 features.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, chi2\n\nch2 = SelectKBest(chi2, k=5000)\ncount_vec_data_NB = ch2.fit_transform(count_vec_data_NB, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's fit the data to Multinomial Naive Bayes model.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(count_vec_data_NB, y, test_size=0.30, random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nmulti_clf = MultinomialNB()\nmulti_clf.fit(X_train, y_train)\n\npredict_NB = multi_clf.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's measure its performance.","metadata":{}},{"cell_type":"code","source":"print(\"Classification Report: \\n\", classification_report(y_test, predict_NB,target_names=['Negative','Positive']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predict_NB))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy: \\n\", accuracy_score(y_test, predict_NB))","metadata":{},"execution_count":null,"outputs":[]}]}